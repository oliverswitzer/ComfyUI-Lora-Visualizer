"""
LoRA Analysis Preprocessor
-------------------------

This module contains helper functions to perform vision-based analysis of
LoRA metadata files using Ollama's multimodal capabilities.  The goal is to
provide richer context for prompt composition by actually examining the
images generated by each LoRA.  When executed at ComfyUI startup, the
preprocessor reads existing ``*.metadata.json`` files, downloads example
images from CivitAI URLs, and uses Ollama's LLaVA vision model to analyze
what the LoRA actually produces.  The analysis includes a summary of when
to use the LoRA and detailed visual analysis of each example image.  Results
are written to ``.analyzed.metadata.json`` files for use by the
LoRAPromptComposerNode.

The structure of the analysis JSON is:

.. code-block:: json

    {
      "when_to_use": "...",
      "example_prompts_and_analysis": [
        {"prompt": "...", "analysis": "..."},
        ...
      ]
    }

The ``when_to_use`` field describes the LoRA's purpose based on actual
visual analysis of generated images, including style, subjects, and scenarios
where it excels.  The ``example_prompts_and_analysis`` field contains each
example prompt and the vision model's detailed analysis of what the
generated image actually shows: subjects, artistic style, lighting,
composition, mood, and distinctive visual characteristics.

The preprocessor runs automatically at ComfyUI startup in a background
thread.  If the analysis file already exists for a given LoRA, the
preprocessor skips reprocessing that file.  The system requires both
the ``requests`` library (for downloading images from CivitAI) and a
running Ollama server with LLaVA or another vision model installed.
Analysis can be disabled via the ``COMFYUI_SKIP_LORA_ANALYSIS``
environment variable.

Example usage::

    from nodes.lora_analysis import analyze_all_loras
    # Vision analysis will automatically use llava:latest
    analyze_all_loras("/path/to/models/loras")

Note that this module does not register any ComfyUI nodes; it is
intended for internal use by other modules.
"""

from __future__ import annotations

import json
import os
from typing import List, Dict, Optional, Any

try:
    import requests  # type: ignore[import]
except Exception:  # pylint: disable=broad-exception-caught
    requests = None

try:
    from server import PromptServer  # type: ignore  # pylint: disable=import-error
except Exception:  # pylint: disable=broad-exception-caught
    PromptServer = None

# Import shared utilities
from .lora_utils import MetadataExtractor
from .lora_analysis.image_analyzer import ImageAnalyzer

# ---------------------------------------------------------------------------
# System prompt for LoRA analysis

_ANALYSIS_SYSTEM_PROMPT = (
    "You are an expert at analyzing machine learning models for image generation.\n"
    "You will be given metadata about a LoRA model including its description and trigger words.\n"
    "You will also be shown example images generated using this LoRA, along with their prompts.\n"
    "Analyze the visual content of the images to understand what this LoRA does.\n\n"
    "Write two fields in JSON format:\n"
    "1. 'when_to_use': A concise paragraph describing the purpose of this LoRA based on the "
    "visual analysis.\n"
    "   Focus on the style, subjects, effects, or scenarios this LoRA is best suited for.\n"
    "2. 'example_prompts_and_analysis': A list of objects where each has 'prompt' and "
    "'analysis' keys.\n"
    "   For each example, include the original prompt and describe what you observe in the "
    "generated image:\n"
    "   subjects, artistic style, lighting, composition, mood, textures, and distinctive "
    "visual characteristics.\n\n"
    "Base your 'when_to_use' summary on patterns you observe across all the example images.\n"
    "Output strictly valid JSON with keys 'when_to_use' and 'example_prompts_and_analysis'.\n"
    "Do not mention the LoRA name or file names; describe content in generic terms."
)


def _call_ollama_for_analysis(
    example_prompts: List[str], model_name: str, api_url: str
) -> Optional[Dict[str, any]]:
    """Send the example prompts to Ollama and parse the analysis response.

    This helper constructs a chat request containing the system prompt and
    the user content (the example prompts), calls the Ollama chat API,
    and attempts to parse the returned content as JSON.  On failure
    (e.g. network issues, JSON decode errors), None is returned.

    Args:
        example_prompts: A list of up to four example prompts from the LoRA metadata.
        model_name: The name of the Ollama model to use.
        api_url: The chat API URL (e.g. "http://localhost:11434/api/chat").

    Returns:
        A dictionary with keys 'when_to_use' and 'example_prompts_and_analysis', or None on failure.
    """
    if requests is None:
        print("LoRA analysis: 'requests' library not available; skipping analysis.")
        return None
    # Build user message containing the example prompts as JSON for clarity.
    user_content = {"example_prompts": example_prompts}
    messages = [
        {"role": "system", "content": _ANALYSIS_SYSTEM_PROMPT},
        {"role": "user", "content": json.dumps(user_content, ensure_ascii=False)},
    ]
    payload = {
        "model": model_name,
        "messages": messages,
        "stream": False,
    }
    try:
        resp = requests.post(api_url, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()
        # Extract assistant content; structure may vary by Ollama version
        if isinstance(data, dict):
            if "message" in data:
                content = data["message"].get("content", "")
            elif "choices" in data and data["choices"]:
                content = data["choices"][0].get("message", {}).get("content", "")
            else:
                content = ""
        else:
            content = ""
        content = content.strip()
        if not content:
            return None
        # Attempt to parse JSON from the assistant content
        try:
            result = json.loads(content)
            when_to_use = result.get("when_to_use")
            examples = result.get("example_prompts_and_analysis")
            if isinstance(when_to_use, str) and isinstance(examples, list):
                return {
                    "when_to_use": when_to_use.strip(),
                    "example_prompts_and_analysis": examples,
                }
            return None
        except (json.JSONDecodeError, KeyError, TypeError) as parse_err:
            print(
                f"LoRA analysis: JSON parse error: {parse_err}; content was: {content}"
            )
            return None
    except Exception as e:
        print(f"LoRA analysis: error contacting Ollama: {e}")
        return None


def _analyze_single_lora_with_vision(
    metadata: Dict[str, Any], api_url: str = "http://localhost:11434/api/chat"
) -> Optional[Dict[str, Any]]:
    """Analyze a single LoRA using vision model to examine actual images.

    Args:
        metadata: Loaded metadata dictionary
        api_url: Ollama API endpoint

    Returns:
        Analysis dict with 'when_to_use' and 'example_prompts_and_analysis' or None
    """
    # Initialize the analyzer
    analyzer = ImageAnalyzer(api_url)
    extractor = MetadataExtractor()

    # Extract examples with prompts and URLs
    examples = extractor.extract_example_data(metadata)
    if not examples:
        print("No example images found in metadata")
        return None

    # Download images
    examples_with_images = analyzer.download_example_images(examples)
    if not examples_with_images:
        print("No images could be downloaded for analysis")
        return None

    # Extract metadata context
    lora_description = extractor.extract_lora_description(metadata)
    trigger_words = extractor.extract_trigger_words(metadata)

    # Perform vision analysis
    print(
        f"Analyzing {len(examples_with_images)} images with {analyzer.DEFAULT_VISION_MODEL}"
    )
    return analyzer.analyze_with_vision(
        examples_with_images, lora_description, trigger_words, _ANALYSIS_SYSTEM_PROMPT
    )


def analyze_all_loras(
    loras_folder: str,
    model_name: str = "nous-hermes2",  # Legacy param, now unused but kept for compatibility  # pylint: disable=unused-argument
    api_url: str = "http://localhost:11434/api/chat",
    status_channel: Optional[str] = "lora_analysis_status",
) -> None:
    """Perform vision-based analysis on all LoRA metadata files in the given folder.

    This function iterates over all ``*.metadata.json`` files in
    ``loras_folder``.  For each metadata file, if a corresponding
    ``*.analyzed.metadata.json`` file is not present, the function
    downloads example images from CivitAI URLs, analyzes them using
    Ollama's vision model (llava), and writes the result.  Status messages are
    emitted via the ComfyUI PromptServer if available.

    Args:
        loras_folder: Path to the folder containing LoRA metadata files.
        model_name: Legacy parameter, no longer used (vision model is auto-selected).
        api_url: URL of the Ollama API endpoint.
        status_channel: Optional message channel used to send status
            updates to the ComfyUI frontend.  If None, no messages are
            sent.
    """
    if not loras_folder or not os.path.isdir(loras_folder):
        return
    # Determine base URL for status messages
    for fname in os.listdir(loras_folder):
        if not fname.endswith(".metadata.json"):
            continue
        meta_path = os.path.join(loras_folder, fname)
        # The analysis file name: insert `.analyzed` before `.metadata.json`
        analyzed_path = meta_path.replace(".metadata.json", ".analyzed.metadata.json")
        # Skip if analysis already exists
        if os.path.exists(analyzed_path):
            continue
        # Load metadata
        meta: Dict[str, any] = {}
        try:
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
        except Exception as e:
            print(f"LoRA analysis: failed to read {fname}: {e}")
            continue
        # Check if we have example images for vision analysis
        extractor = MetadataExtractor()
        examples = extractor.extract_example_data(meta)
        if not examples:
            # Nothing to analyze; write empty analysis to avoid reprocessing
            try:
                with open(analyzed_path, "w", encoding="utf-8") as out:
                    json.dump({}, out)
            except Exception:
                pass
            continue

        # Inform user that vision analysis is starting
        if status_channel and PromptServer is not None:
            try:
                PromptServer.instance.send_sync(
                    status_channel,
                    {
                        "status": (
                            f"Analyzing LoRA '{fname}' with vision model (downloading images)..."
                        )
                    },
                )
            except Exception:
                pass

        # Use vision analysis instead of text-only analysis
        result = _analyze_single_lora_with_vision(meta, api_url)
        if result is None:
            # Write empty analysis to avoid repeated attempts
            try:
                with open(analyzed_path, "w", encoding="utf-8") as out:
                    json.dump({}, out)
            except Exception:
                pass
            continue
        # Write analysis to file
        try:
            with open(analyzed_path, "w", encoding="utf-8") as out:
                json.dump(result, out, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"LoRA analysis: failed to write {analyzed_path}: {e}")
            continue
        # Notify completion
        if status_channel and PromptServer is not None:
            try:
                PromptServer.instance.send_sync(
                    status_channel, {"status": f"Finished analyzing LoRA '{fname}'."}
                )
            except Exception:
                pass
